seed: 0
plot: true

# Meta-learning parameters
inner_stepsize: 0.02  # stepsize in inner SGD
outerstepsize: 0.1  # stepsize of outer optimization, i.e., meta-optimization
outer_steps: 30000  # number of outer updates; each iteration we sample one task and update on it

# Task generation parameters
x_min: -5
x_max: 5
x_points: 50
ntrain: 10  # Size of training minibatches

# Model parameters
hidden_dim: 64
activation: tanh  # tanh or relu
intermediate_dim: 128
hyper_input_dim: 128
hypernet_lr: 1e-4  # Learning rate for hypernetwork optimizer
hypernet_weight_decay: 0.0  # Weight decay for hypernetwork optimizer

# Device
device: cpu

# Logging
wandb:
  log: false
  project: hyper_reptile
  entity: ${oc.env:WANDB_ENTITY,null}
  run_name: ${oc.env:WANDB_RUN_NAME,null}
  run_id: ${oc.env:WANDB_RUN_ID,null}
  group: ${oc.env:WANDB_GROUP,null}
  tags: ${oc.env:WANDB_TAGS,null}
  notes: ${oc.env:WANDB_NOTES,null}

# Output
assets_dir: assets
save_plots: true
plot_interval: 1000  # Plot every N iterations
inner_plot_steps: 32
plot_inner_interval: 8

pbar_interval: 100  # Update progress bar every N iterations

mode: reptile  # Options: 'reptile' or 'hypernet'

num_tasks: 10  # Number of tasks for hypernet
val_interval: 1000  # Validation interval for hypernet
input_cond: false  # Whether to use input conditioning in hypernet